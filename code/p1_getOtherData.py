"""
Script for importing data from non-osm sources
- generate_elevation_file: get elevation data from Open Elevation API
- generate_bike_traffic_volume_file: get bike traffic volume data from Munich Open Data
- generate_accidents_file: get bike accidents data from Munich Open Data
- generate_district_centers_file: get district centers data from Munich Open Data

Inputs:
- configFile.py: configuration file with all values
    - version: version of the project
    - city_info: dictionary with city information
    - p1_result_filepath: filepath for geopackage file with the osm network
    - elev_filepath: filepath for elevation data
    - X accidents_filepath: filepath for bike accidents data
    - X district_centers_input_filepath: filepath for district centers data
    - X district_centers_output_filepath: filepath for district centers data
    - X bike_traffic_vol_output_filepath: filepath for bike traffic volume data    
Outputs:
- elevation data (saved to elev_filepath)
- X bike traffic volume data (saved to bike_traffic_vol_output_filepath)
- X bike accidents data (saved to accidents_filepath)
- X district centers data (saved to district_centers_output_filepath)
"""
# %%

#######################################
import pandas as pd
import geopandas as gpd
import requests
import re
import os
import os.path

# load all values in configFile.py
import configFile 

# VALUES #############################################################################
version = configFile.version
city = configFile.city_info['city']

p1_result_fp = configFile.p1_result_filepath
elev_fp = configFile.elev_filepath
# DISABLED: accidents_fp = configFile.accidents_filepath
# DISABLED: district_centers_input_fp = configFile.district_centers_input_filepath
# DISABLED: district_centers_output_fp = configFile.district_centers_output_filepath
# DISABLED: bike_traffic_vol_output_fp = configFile.bike_traffic_vol_output_filepath

# FUNCTIONS #############################################################################

def get_elevation(location):
    """
    Extracts elevation von Open Elavation API for all nodes
    Because of limitation, data is requested in packages of 2000 nodes
    Args:
        @location
    Returns:
        elevation in the provided location
    """
    url = f'https://api.open-elevation.com/api/v1/lookup?'
    if location:
        r = requests.post(url, json={'locations': location})
        print(r)
        return r.json()['results']
    return []

def generate_elevation_file(input_file=p1_result_fp,
                            output_file=elev_fp):
    """
    Extracts elevation data from Open Elevation API
    Args:
        @input_file, default:"./output_data/Munich/osmnx/bike_network_osmnx_TEST.gpkg" generated by  p1_getOSMNetwork.py
        @output_file, default:"./output_data/Munich/add_data/elevations.json"
    """
    if not os.path.isfile(input_file):
        raise ValueError('Input file {} doesn''t exist. Use p1_getOSMNetwork.py to generate it.'.format(input_file))
    # os.path.isfile(fname) # checks if file exists
    # raise ValueError('A very specific bad thing happened.')
    # gdf_edges_bike = gpd.read_file(input_file, layer='edges')
    gdf_nodes_bike = gpd.read_file(input_file, layer='nodes')
    if gdf_nodes_bike is None or len(gdf_nodes_bike) == 0:
        raise ValueError('Input file {} is empty. Use p1_getOSMNetwork.py to generate it.'.format(input_file))
    j = []
    a = []
    idx = []
    for count, node in enumerate(gdf_nodes_bike.itertuples(), 1):
        # if the querying is interrupted, restart it where it left off
        # e.g. if count=82000, continue with a counter greater than that
        # if count>=82000:
        j.append({'latitude': node.y, 'longitude': node.x})
        idx.append(node.Index)
        if not count % 2000:
            a.extend([{'idx': idx[c], 'elevation': tmp['elevation']} for c, tmp in enumerate(get_elevation(j))])
            j = []
            idx = []
    a.extend([{'idx': idx[c], 'elevation': tmp['elevation']} for c, tmp in enumerate(get_elevation(j))])
    if len(a) == 0:
        raise ValueError('Missing elevation data')
    # Write elevation in json-File
    output_directory = os.path.dirname(output_file)
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)
    with open(output_file, 'w') as fp:
        fp.write(str(a))
    print("{} rows of elevation data written to file {}".format(len(a), output_file))

def generate_bike_traffic_volume_file(output_file):
    """
    Extracts bike traffic volume from Munich Open Data https://opendata.muenchen.de/
    Source: Radverkerszählnugen München
    Args: @output_file, default: "./output_data/Munich/add_data/bike_traffic_volume.json"
    Returns: Bool, True if extraction was successfull, false else
    """
    maxi = {}
    api_return = requests.get(
        'https://opendata.muenchen.de/api/3/action/datastore_search?resource_id=211e882d-fadd-468a-bf8a-0014ae65a393&limit=500')
    if api_return.status_code == 200:
        for tmp in api_return.json()['result']['records']:
            maxi[tmp['zaehlstelle']] = {'richtung_1': 0, 'r1_date': None, 'richtung_2': 0, 'r2_date': None, 'gesamt': 0,
                                        'g_date': None}
    else:
        print('ERROR 4711', api_return)
        return False

    api_return = requests.get('https://opendata.muenchen.de/api/3/action/package_search?fq=tags:Fahrrad&rows=1000')
    if api_return.status_code == 200:
        # ids = []
        for tmp in api_return.json()['result']['results']:
            for xyz in tmp['resources']:
                if xyz['name'][0] == 'T':
                    t = re.search('(\w+)\W+(\d+)', xyz['name'])
                    # ids.append({'year': t.group(2), 'month': t.group(1), 'id': xyz['id']})

                    api_subreturn_url = 'https://opendata.muenchen.de/api/3/action/datastore_search?resource_id=' + xyz[
                        'id'] + '&limit=10000'
                    api_subreturn = requests.get(api_subreturn_url)
                    if api_subreturn.status_code == 200:
                        for tmp in api_subreturn.json()['result']['records']:
                            # print(tmp)
                            if int(tmp['richtung_1']) > maxi[tmp['zaehlstelle']]['richtung_1']:
                                maxi[tmp['zaehlstelle']]['richtung_1'] = int(tmp['richtung_1'])
                                maxi[tmp['zaehlstelle']]['r1_date'] = tmp['datum']
                            if int(tmp['richtung_2']) > maxi[tmp['zaehlstelle']]['richtung_2']:
                                maxi[tmp['zaehlstelle']]['richtung_2'] = int(tmp['richtung_2'])
                                maxi[tmp['zaehlstelle']]['r2_date'] = tmp['datum']
                            if int(tmp['gesamt']) > maxi[tmp['zaehlstelle']]['gesamt']:
                                maxi[tmp['zaehlstelle']]['gesamt'] = int(tmp['gesamt'])
                                maxi[tmp['zaehlstelle']]['g_date'] = tmp['datum']
                    else:
                        print('########## ERROR 1337', api_subreturn, api_subreturn_url, t)
                        # return False
    else:
        print('ERROR 1234', api_return)
        return False
    output_directory = os.path.dirname(output_file)
    if not os.path.exists(output_directory):
        os.makedirs(output_directory)
    with open(output_file, 'w') as fp:
        fp.write(str(maxi))
    return True

def generate_accidents_file(output_file):
    """
    Generate bike accidents data for Munich
    Args:
        @output_file, default: "./output_data/Munich/add_data/bike_accidents.gpkg"
    Returns: None
    """
    # All bike accidents in Munich from 2016-2020
    files = [file for file in os.listdir('./input_data/' + city + '/accidents')]
    df_accidents = pd.DataFrame()
    for file in files:
        # here data types of cols should be specified
        df = pd.read_csv('./input_data/Munich/accidents/' + file, sep=';', decimal=',', low_memory=False)
        df = df[(df['ULAND'] == 9) & (df['UKREIS'] == 62) & (df['IstRad'] == 1) & (df['UREGBEZ'] == 1)]
        df_accidents = pd.concat([df_accidents, df])
    # Creating Geodataframe from Dateframe
    gdf_accidents = gpd.GeoDataFrame(df_accidents,
                                     geometry=gpd.points_from_xy(df_accidents.XGCSWGS84, df_accidents.YGCSWGS84,
                                                                 crs="EPSG:4326"))
    gdf_accidents = gdf_accidents.drop(columns=['LINREFX', 'LINREFY', 'XGCSWGS84', 'YGCSWGS84'])

    # Creating Geopackage
    gdf_accidents.to_file(output_file, driver="GPKG", layer='accidents')
    print("{} accidents written to file {}".format(df_accidents.shape[0], output_file))

def generate_district_centers_file(input_file, output_file):
    """
    Generate district centers data for Munich 
    Args:
        input_file, default: './input_data/Munich/zentren/zentren_stadtteile.csv'
        output_file: default: "./output_data/Munich/add_data/zentren.gpkg"
    Returns: None
    """
    # Import zentren
    df_zentren = pd.read_csv(input_file, sep=';', decimal='.')
    # Generate GeoDataFrame
    gdf_zentren = gpd.GeoDataFrame(df_zentren, geometry=gpd.geoseries.from_wkb(df_zentren.geom), crs="EPSG:4326")
    gdf_zentren = gdf_zentren.drop(gdf_zentren.columns[[0, 1]], axis=1)
    gdf_zentren = gdf_zentren.drop(gdf_zentren.iloc[:, 11:135], axis=1)

    # Creating Geopackage
    gdf_zentren.to_file(output_file, driver="GPKG", layer='zentren')
    print("{} district centers written to file {}".format(gdf_zentren.shape[0], output_file))


def main(elevation=True, bike_traffic=False, accidents=False, district_centers=False):
    if elevation:
        generate_elevation_file()
    # if bike_traffic:
    #     generate_bike_traffic_volume_file(bike_traffic_vol_output_fp)
    # if accidents:
    #     generate_accidents_file(accidents_fp)
    # if district_centers:
    #     generate_district_centers_file(input_file=district_centers_input_fp,output_file=district_centers_output_fp)

if __name__ == "__main__":
    main()


############################################
# Notes
############################################

# TODO: add cycle path width generation from .shp (in a different location currently)


############################################
# Air emissions
############################################
# PART 1 - NOTE: is this cell necessary?
'''
# Import and calculate average/max NO2 immissions
files = [file for file in os.listdir('./input_data/air_immissions/NO2')]
stations_NO2_immissions = ['München/Allach','München/Johanneskirchen','München/Landshuter Allee', 'München/Lothstraße','München/Stachus']
cols = stations_NO2_immissions.copy()
cols.insert(0,'Zeitpunkt')
df_NO2_immissions_part = pd.DataFrame(index=stations_NO2_immissions)
df_NO2_immissions_max = pd.DataFrame(index=stations_NO2_immissions)

# Öffne excel file and make a data frame
for file in files:
    df_NO2_immissions = pd.DataFrame(pd.read_excel('./input_data/air_immissions/NO2/' + file ,header=1,index_col=None ,usecols=cols))

    # Replace komische Werte durch NAN in den Stations columns
    for station in stations_NO2_immissions:
        print(station)
        df_NO2_immissions.replace({station:r'.*\D.*'},value=np.nan, regex=True, inplace=True)
        # control = df_NO2_immissions[station].sum(axis=1)
        # print(control)
        # s = 0
    # Summiere nur Werte mit zeitpunkt zwischen 6:00 und 22:00 Uhr
    # if 5 < int(df_NO2_immissions.Zeitpunkt[11:13]) < 23:
    #     df_NO2_immissions[station]
    #s = df_NO2_immissions[(df_NO2_immissions.Zeitpunkt == '01.01.2019 01:00') (df_NO2_immissions.Zeitpunkt == '01.01.2019 02:00')].sum()
    print('----')
    print(df_NO2_immissions)
    print(df_NO2_immissions['München/Allach'][0])
    print('---')
    #for k, v in df_NO2_immissions['Zeitpunkt'].items():
    #    print(k, v)
    s = 0
    for k,v in df_NO2_immissions['München/Allach'].items():
        try:
            if 5 < int(df_NO2_immissions['Zeitpunkt'][k][11:13]) < 23:
                print(k, v)
                s += v
        except ValueError:
            pass
    print(s)
    #print(df_NO2_immissions.Zeitpunkt.items())

    #print([v for v in df_NO2_immissions.Zeitpunkt if 5 < int(v[11:13]) < 23])
    break
    df_NO2_immissions.drop()
    s = sum([v for v,k in df_NO2_immissions.Zeitpunkt.items() if 5 < k[11:13]< 23])
    print(s)
    break

'''
##################################
# PART 2
'''
# Import and calculate average/max NO2 immissions
files = [file for file in os.listdir('./input_data/air_immissions/NO2')]
stations_NO2_immissions = ['München/Allach', 'München/Johanneskirchen', 'München/Landshuter Allee',
                           'München/Lothstraße', 'München/Stachus']
df_NO2_immissions_part = pd.DataFrame(index=stations_NO2_immissions)
df_NO2_immissions_max = pd.DataFrame(index=stations_NO2_immissions)
for file in files:
    df_NO2_immissions = pd.DataFrame(pd.read_excel('./input_data/air_immissions/NO2/' + file, header=1, index_col=None,
                                                   usecols=stations_NO2_immissions))
    df_NO2_immissions.replace(to_replace=r'.*\D.*', value=np.nan, regex=True, inplace=True)

    for station in stations_NO2_immissions:
        s = df_NO2_immissions[station].sum()
        n = df_NO2_immissions[station].isna().sum()
        l = len(df_NO2_immissions[station])
        q = l - n
        p = s / q
        m = df_NO2_immissions[station].max()
        df_NO2_immissions_part.loc[[station], ['part' + '/' + file]] = p
        df_NO2_immissions_max.loc[[station], ['max' + '/' + file]] = m
    print(file)

df_NO2_immissions_new = pd.DataFrame(index=stations_NO2_immissions)
df_NO2_immissions_new['Durchschnitt'] = df_NO2_immissions_part.sum(axis=1) / len(files)
df_NO2_immissions_new['Maximal'] = df_NO2_immissions_max.max(axis=1)

df_NO2_immissions_new.head()
df_NO2_immissions_new.to_csv("./output_data/" + city + "/add_data/NO2.csv")
'''
##################################
# PART 3
'''
# Import and calculate average/max O3 immissions
files = [file for file in os.listdir('./input_data/air_immissions/O3')]
stations_O3_immissions = ['München/Allach', 'München/Johanneskirchen', 'München/Lothstraße', 'München/Stachus']
df_O3_immissions_part = pd.DataFrame(index=stations_O3_immissions)
df_O3_immissions_max = pd.DataFrame(index=stations_O3_immissions)

for file in files:
    df_O3_immissions = pd.DataFrame(pd.read_excel('./input_data/air_immissions/O3/' + file, header=1, index_col=None,
                                                  usecols=stations_O3_immissions))
    df_O3_immissions.replace(to_replace=r'.*\D.*', value=np.nan, regex=True, inplace=True)

    for station in stations_O3_immissions:
        s = df_O3_immissions[station].sum()
        n = df_O3_immissions[station].isna().sum()
        l = len(df_O3_immissions[station])
        q = l - n
        p = s / q
        m = df_O3_immissions[station].max()
        df_O3_immissions_part.loc[[station], ['part' + '/' + file]] = p
        df_O3_immissions_max.loc[[station], ['max' + '/' + file]] = m
    print(file)

df_O3_immissions_new = pd.DataFrame(index=stations_O3_immissions)
df_O3_immissions_new['Durchschnitt'] = df_O3_immissions_part.sum(axis=1) / len(files)
df_O3_immissions_new['Maximal'] = df_O3_immissions_max.max(axis=1)

df_O3_immissions_new.head()
df_O3_immissions_new.to_csv("./output_data/" + city + "/add_data/O3.csv")
'''
##################################
# PART 4
'''
# Import and calculate average/max PM10 immissions
files = [file for file in os.listdir('./input_data/air_immissions/PM10')]
stations_PM10_immissions = ['München/Johanneskirchen', 'München/Landshuter Allee', 'München/Lothstraße',
                            'München/Stachus']
df_PM10_immissions_part = pd.DataFrame(index=stations_PM10_immissions)
df_PM10_immissions_max = pd.DataFrame(index=stations_PM10_immissions)

for file in files:
    df_PM10_immissions = pd.DataFrame(
        pd.read_excel('./input_data/air_immissions/PM10/' + file, header=1, index_col=None,
                      usecols=stations_PM10_immissions))
    df_PM10_immissions.replace(to_replace=r'.*\D.*', value=np.nan, regex=True, inplace=True)

    for station in stations_PM10_immissions:
        s = df_PM10_immissions[station].sum()
        n = df_PM10_immissions[station].isna().sum()
        l = len(df_PM10_immissions[station])
        q = l - n
        p = s / q
        m = df_PM10_immissions[station].max()
        df_PM10_immissions_part.loc[[station], ['part' + '/' + file]] = p
        df_PM10_immissions_max.loc[[station], ['max' + '/' + file]] = m
    print(file)

df_PM10_immissions_new = pd.DataFrame(index=stations_PM10_immissions)
df_PM10_immissions_new['Durchschnitt'] = df_PM10_immissions_part.sum(axis=1) / len(files)
df_PM10_immissions_new['Maximal'] = df_PM10_immissions_max.max(axis=1)

df_PM10_immissions_new.head()
df_PM10_immissions_new.to_csv("./output_data/" + city + "/add_data/PM10.csv")
'''
